# -*- coding: utf-8 -*-
"""Amazon_hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AgrR90Cf-sat4SDeoByxaIJNtqiN4Grd
"""

Data Processing

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/MyDrive/Input "

"""Extract Zip File"""

import zipfile

zip_path = "/content/drive/MyDrive/Input /68e8d1d70b66d_student_resource.zip"
extract_path = "/content/drive/MyDrive/Output"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Files extracted successfully!")
!ls /content/drive/MyDrive/Output

"""Load data in pandas"""

import pandas as pd

train_df = pd.read_csv("/content/drive/MyDrive/Output/student_resource/dataset/train.csv")
test_df = pd.read_csv("/content/drive/MyDrive/Output/student_resource/dataset/test.csv")

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)

train_df.head()

"""Confirm File and Direction"""

import os

# Change these paths if needed
DATA_ROOT = "/content/drive/MyDrive/Output/student_resource/dataset"
print("Contents of data directory:")
!ls {DATA_ROOT}

# Paths to files
train_path = os.path.join(DATA_ROOT, "train.csv")
test_path = os.path.join(DATA_ROOT, "test.csv")

# Load
import pandas as pd
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

print("Train:", train_df.shape)
print("Test:", test_df.shape)
train_df.head()

"""Basic missing value handling & cleaning template"""

# Fill missing text & image link
train_df['catalog_content'] = train_df['catalog_content'].fillna("")
test_df['catalog_content'] = test_df['catalog_content'].fillna("")

train_df['image_link'] = train_df['image_link'].fillna("")
test_df['image_link'] = test_df['image_link'].fillna("")

# Add a flag for image presence
train_df['has_image'] = (train_df['image_link'] != "").astype(int)
test_df['has_image'] = (test_df['image_link'] != "").astype(int)

# Quick sanity
train_df[['sample_id', 'catalog_content', 'image_link', 'has_image']].head(5)

"""Text preprocessing and extraction functions"""

import re

def clean_text(s: str) -> str:
    s = s.lower()
    # remove URLs, HTML tags
    s = re.sub(r"http\S+|www\S+|<.*?>", " ", s)
    # keep alphanumeric + spaces
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    # collapse multiple spaces
    s = re.sub(r"\s+", " ", s).strip()
    return s

def extract_ipq(s: str):
    s = s.lower()
    # “6 pack”, “pack of 4”, etc.
    m = re.search(r'(\d+)\s*(pack|packs|pcs|pieces|count|units|pk|packs?)', s)
    if m:
        return int(m.group(1))
    # fallback numeric
    m2 = re.search(r'(\d+)\b', s)
    if m2:
        return int(m2.group(1))
    return None

def extract_quantity(s: str):
    # returns (value, unit) or (None, None)
    s = s.lower()
    m = re.search(r'(\d+)\s*(ml|g|kg|l)\b', s)
    if m:
        return int(m.group(1)), m.group(2)
    return None, None

for df in [train_df, test_df]:
    df['clean_text'] = df['catalog_content'].map(clean_text)
    df['ipq'] = df['clean_text'].map(extract_ipq)
    qty_val, qty_unit = zip(*df['clean_text'].map(extract_quantity))
    df['qty_val'] = qty_val
    df['qty_unit'] = qty_unit
    # text stats
    df['char_len'] = df['clean_text'].str.len()
    df['word_count'] = df['clean_text'].str.split().map(len)
    df['num_digits'] = df['clean_text'].map(lambda x: sum(c.isdigit() for c in x))

# See result
train_df[['sample_id','clean_text','ipq','qty_val','qty_unit','char_len','word_count','num_digits']].head(5)

"""Visualize / Inspect distributions"""

import matplotlib.pyplot as plt
import seaborn as sns

# Price distribution (train)
sns.histplot(train_df['price'], bins=100, log_scale=True)
plt.title("Train Price Distribution (log scale)")
plt.show()

# ipq vs price
sns.scatterplot(data=train_df, x='ipq', y='price')
plt.title("IPQ vs Price")
plt.show()

# qty_val / unit frequencies
print(train_df['qty_unit'].value_counts().head(20))

"""Prepare target transformation"""

import numpy as np

train_df['price_log1p'] = np.log1p(train_df['price'])

"""Text feature extraction (baseline TF-IDF)"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1,2),
    analyzer='word'
)

# Fit on train clean_text
X_text_train = tfidf.fit_transform(train_df['clean_text'])
X_text_test = tfidf.transform(test_df['clean_text'])

print("Text feature dims:", X_text_train.shape, X_text_test.shape)

"""Numeric & categorical feature encoding"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# Numeric features list
num_cols = ['ipq','qty_val','char_len','word_count','num_digits','has_image']

# Impute missing numeric
imputer = SimpleImputer(strategy='constant', fill_value=0)
X_num_train = imputer.fit_transform(train_df[num_cols])
X_num_test = imputer.transform(test_df[num_cols])

# Scale numeric
scaler = StandardScaler()
X_num_train = scaler.fit_transform(X_num_train)
X_num_test = scaler.transform(X_num_test)

# Categorical encoding for qty_unit
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)

X_cat_train = ohe.fit_transform(train_df[['qty_unit']].fillna(''))
X_cat_test = ohe.transform(test_df[['qty_unit']].fillna(''))

print("Numeric dims:", X_num_train.shape, "Cat dims:", X_cat_train.shape)

"""Combine features (sparse + dense)"""

from scipy.sparse import hstack, csr_matrix

# Convert dense numeric+cat to sparse
X_dense_train = csr_matrix(
    np.hstack([X_num_train, X_cat_train])
)
X_dense_test = csr_matrix(
    np.hstack([X_num_test, X_cat_test])
)

# Final feature matrices
X_train = hstack([X_text_train, X_dense_train])
X_test = hstack([X_text_test, X_dense_test])

print("Final feature dims:", X_train.shape, X_test.shape)

"""Baseline model training (LightGBM)"""

# Updated LightGBM baseline model training
import lightgbm as lgb
from lightgbm import early_stopping, log_evaluation
from sklearn.model_selection import KFold
import numpy as np
import pandas as pd

# Target (log-transformed price)
y = train_df['price_log1p'].values

# Initialize KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Placeholders for predictions
preds_test_log = np.zeros(X_test.shape[0])
oof_preds = np.zeros(X_train.shape[0])

# Training loop
for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):
    print(f"\n================ Fold {fold+1} / {kf.n_splits} ================")

    X_tr, X_val = X_train[tr_idx], X_train[val_idx]
    y_tr, y_val = y[tr_idx], y[val_idx]

    dtrain = lgb.Dataset(X_tr, label=y_tr)
    dvalid = lgb.Dataset(X_val, label=y_val)

    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'learning_rate': 0.05,
        'num_leaves': 128,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbosity': -1,
        'seed': 42
    }

    # ✅ Updated method for early stopping and logging
    model = lgb.train(
        params,
        dtrain,
        valid_sets=[dvalid],
        callbacks=[
            early_stopping(stopping_rounds=100),
            log_evaluation(100)
        ]
    )

    # Save out-of-fold predictions
    oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)
    # Average test predictions across folds
    preds_test_log += model.predict(X_test, num_iteration=model.best_iteration) / kf.n_splits

# Convert back to original price scale
preds_test = np.expm1(preds_test_log)
oof_pred_price = np.expm1(oof_preds)

# Create submission DataFrame
submission = pd.DataFrame({
    'sample_id': test_df['sample_id'],
    'price': preds_test
})

# Save to CSV
submission.to_csv("test_out_baseline.csv", index=False, float_format="%.6f")

print("\nBaseline training complete! File saved as test_out_baseline.csv")

"""Evaluate OOF & error (on train)"""

def smape(y_true, y_pred):
    return np.mean(np.abs(y_pred - y_true) / ((np.abs(y_pred) + np.abs(y_true)) / 2.0))

smape_score = smape(train_df['price'].values, oof_pred_price)
print(f"OOF SMAPE score: {smape_score:.4f}")

"""Sample Output in CSV file"""

import pandas as pd

df = pd.read_csv("test_out_baseline.csv")
print(df.info())
print(df.head())
print("Rows:", len(df))
print("Any missing values?", df.isna().sum().any())
print("All positive prices?", (df['price'] > 0).all())